{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2bfdb3",
   "metadata": {},
   "source": [
    "### Document structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0506dfd-cb13-4d24-b084-2b7fd9ea3406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529d8383-3039-4b4a-bdc7-a83dc9f66f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\n",
    "    page_content = \"This is a sample text content\",\n",
    "    metadata = {\n",
    "        \"source\":\"examples\",\n",
    "        \"pages\" : 1,\n",
    "        \"author\" : \"ABC\"\n",
    "    } # metadata is useful for applying them as filters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c3be5d5-3c98-41a2-b64a-912ad477afb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'examples', 'pages': 1, 'author': 'ABC'}, page_content='This is a sample text content')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03dd3116-d7ad-48aa-85df-b2e310ea6314",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a simple txt file \n",
    "import os \n",
    "os.makedirs(\"data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2637ca77-9fc0-49c3-9573-e1f3820b32c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/text_files/python_intro.txt\n",
      "data/text_files/machine_learning.txt\n",
      "Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "\n",
    "for filepath,content in sample_texts.items():\n",
    "    print(filepath)\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a1843e-cbc8-4e49-b352-b3c30ea2c532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "Python Programming Introduction\n",
      "\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
      "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
      "programming languages in the world.\n",
      "\n",
      "Key Features:\n",
      "- Easy to learn and use\n",
      "- Extensive standard library\n",
      "- Cross-platform compatibility\n",
      "- Strong community support\n",
      "\n",
      "Python is widely used in web development, data science, artificial intelligence, and automation.\n"
     ]
    }
   ],
   "source": [
    "## TextLoader\n",
    "from langchain.document_loaders.text import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/text_files/python_intro.txt\")\n",
    "documents = loader.load()\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65630553",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DirectoryLoader' from 'langchain_core.document_loaders' (/Users/kiranm/Desktop/MySpace/rag/rag_project/rag_env/lib/python3.9/site-packages/langchain_core/document_loaders/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## DirectoryLoader\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DirectoryLoader\n\u001b[1;32m      3\u001b[0m dir_loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/text_files\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      4\u001b[0m                              glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      5\u001b[0m                              loader_cls\u001b[38;5;241m=\u001b[39mTextLoader,\n\u001b[1;32m      6\u001b[0m                              loader_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      7\u001b[0m                              show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# set to True to see progress bar (requires tqdm package)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m                              )\n\u001b[1;32m     10\u001b[0m documents \u001b[38;5;241m=\u001b[39m dir_loader\u001b[38;5;241m.\u001b[39mload()\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DirectoryLoader' from 'langchain_core.document_loaders' (/Users/kiranm/Desktop/MySpace/rag/rag_project/rag_env/lib/python3.9/site-packages/langchain_core/document_loaders/__init__.py)"
     ]
    }
   ],
   "source": [
    "## DirectoryLoader\n",
    "from langchain_core.document_loaders import DirectoryLoader\n",
    "dir_loader = DirectoryLoader(\"data/text_files\", \n",
    "                             glob=\"*.txt\", \n",
    "                             loader_cls=TextLoader,\n",
    "                             loader_kwargs={\"encoding\":\"utf-8\"},\n",
    "                             show_progress=False # set to True to see progress bar (requires tqdm package)\n",
    "                             )\n",
    "\n",
    "documents = dir_loader.load()\n",
    "documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bc4c9bf-3cbe-4a85-845b-6edb4fee36d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## PDF loader\n",
    "# from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "# from pathlib import Path\n",
    "\n",
    "# dir_loader = DirectoryLoader(\"data/pdf_files\",\n",
    "#                                 glob=\"*.pdf\", \n",
    "#                                 loader_cls=PyMuPDFLoader,\n",
    "#                                 show_progress=False # set to True to see progress bar (requires tqdm package)\n",
    "#                                 )\n",
    "\n",
    "# pdf_documents = dir_loader.load()\n",
    "# print(f\"Number of PDF documents: {len(pdf_documents)}\")\n",
    "# # pdf_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6b891d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiranm/Desktop/MySpace/rag/rag_project/rag_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efb3aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files to process\n",
      "\n",
      "Processing: vLLM_paper.pdf\n",
      "  ✓ Loaded 16 pages\n",
      "\n",
      "Processing: HumanEval_on_LLMs.pdf\n",
      "  ✓ Loaded 6 pages\n",
      "\n",
      "Total documents loaded: 22\n"
     ]
    }
   ],
   "source": [
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "pdf_documents = process_all_pdfs(\"data/pdf_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435b425f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5428"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_documents[3].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3320399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4679"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pdf_documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14679844",
   "metadata": {},
   "source": [
    "## Chunk the pdf documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa6ef693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Example: pdf_documents is already loaded like this:\n",
    "# pdf_documents = [Document(page_content=\"...\"), Document(page_content=\"...\")]\n",
    "\n",
    "# Initialize a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # number of characters per chunk\n",
    "    chunk_overlap=200,    # overlap between chunks\n",
    "    length_function=len,  # function to measure length\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # order of splitting preference\n",
    ")\n",
    "\n",
    "# Split into chunks\n",
    "docs_chunks = text_splitter.split_documents(pdf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a869381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original docs: 22\n",
      "Chunked docs: 144\n",
      "page_content='Efficient Memory Management for Large Language\n",
      "Model Serving with PagedAttention\n",
      "Woosuk Kwon1,∗ Zhuohan Li1,∗ Siyuan Zhuang1 Ying Sheng1,2 Lianmin Zheng1 Cody Hao Yu3\n",
      "Joseph E. Gonzalez1 Hao Zhang4 Ion Stoica1\n",
      "1UC Berkeley 2Stanford University 3Independent Researcher 4UC San Diego\n",
      "Abstract\n",
      "High throughput serving of large language models (LLMs)\n",
      "requires batching sufficiently many requests at a time. How-\n",
      "ever, existing systems struggle because the key-value cache\n",
      "(KV cache) memory for each request is huge and grows\n",
      "and shrinks dynamically. When managed inefficiently, this\n",
      "memory can be significantly wasted by fragmentation and\n",
      "redundant duplication, limiting the batch size. To address\n",
      "this problem, we propose PagedAttention, an attention al-\n",
      "gorithm inspired by the classical virtual memory and pag-\n",
      "ing techniques in operating systems. On top of it, we build\n",
      "vLLM, an LLM serving system that achieves (1) near-zero\n",
      "waste in KV cache memory and (2) flexible sharing of KV' metadata={'producer': 'pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'creator': 'LaTeX with acmart 2023/03/26 v1.89a Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2023-09-13T00:44:07+00:00', 'moddate': '2023-09-13T00:44:07+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Efficient Memory Management for Large Language Model Serving with PagedAttention', 'trapped': '/False', 'source': 'data/pdf_files/vLLM_paper.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'source_file': 'vLLM_paper.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original docs: {len(pdf_documents)}\")\n",
    "print(f\"Chunked docs: {len(docs_chunks)}\")\n",
    "print(docs_chunks[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc73bf7",
   "metadata": {},
   "source": [
    "### Embedding and VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0160b8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fac27418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model all-MiniLM-L6-v2 loaded successfully. Embdedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x31e977310>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager : \n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model {self.model_name} loaded successfully. Embdedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded. Call _load_model() first.\")\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "    ## Initialise the embedding manager\n",
    "embedding_manager = EmbeddingManager(model_name=\"all-MiniLM-L6-v2\")\n",
    "embedding_manager\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28939886",
   "metadata": {},
   "source": [
    "### VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecb84b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized at data/vector_store with collection pdf_documents\n",
      "Existing documents in collectio : 144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x31f6db880>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"data/vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "        \n",
    "    def _initialize_store(self):\n",
    "        \n",
    "        try:   \n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF Document Embdeddings for RAG\"})\n",
    "            \n",
    "            print(f\"Vector store initialized at {self.persist_directory} with collection {self.collection_name}\")\n",
    "            print(f\"Existing documents in collectio : {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \n",
    "        if len(documents) != embeddings.shape[0]:\n",
    "            raise ValueError(\"Number of documents and embeddings must match.\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to the vector store...\")\n",
    "        \n",
    "        # prepare data for chromadb\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i,(doc,embedding) in enumerate(zip(documents,embeddings)):\n",
    "            # generate unique id\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # prepare metadata\n",
    "            metadata = dict(doc.metadata) if doc.metadata else {}\n",
    "            metadata[\"doc_index\"] = i\n",
    "            metadata[\"content_length\"] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "            \n",
    "        # add to collection\n",
    "        try :\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text,\n",
    "                embeddings=embeddings_list\n",
    "            )\n",
    "            print(f\"Documents added successfully. Total documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise e    \n",
    "        \n",
    "        \n",
    "vectorstore = VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80183deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4679,\n",
       " 5337,\n",
       " 5572,\n",
       " 5428,\n",
       " 4944,\n",
       " 5615,\n",
       " 5690,\n",
       " 5873,\n",
       " 5176,\n",
       " 4197,\n",
       " 4109,\n",
       " 4537,\n",
       " 5463,\n",
       " 6904,\n",
       " 7621,\n",
       " 788,\n",
       " 5234,\n",
       " 5099,\n",
       " 5163,\n",
       " 4841,\n",
       " 6437,\n",
       " 1981]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(pdf_documents[i].page_content) for i in range(len(pdf_documents))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1070a7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Efficient Memory Management for Large Language\\nModel Serving with PagedAttention\\nWoosuk Kwon1,∗ Zhuohan Li1,∗ Siyuan Zhuang1 Ying Sheng1,2 Lianmin Zheng1 Cody Hao Yu3\\nJoseph E. Gonzalez1 Hao Zhang4 Ion Stoica1\\n1UC Berkeley 2Stanford University 3Independent Researcher 4UC San Diego\\nAbstract\\nHigh throughput serving of large language models (LLMs)\\nrequires batching sufficiently many requests at a time. How-\\never, existing systems struggle because the key-value cache\\n(KV cache) memory for each request is huge and grows\\nand shrinks dynamically. When managed inefficiently, this\\nmemory can be significantly wasted by fragmentation and\\nredundant duplication, limiting the batch size. To address\\nthis problem, we propose PagedAttention, an attention al-\\ngorithm inspired by the classical virtual memory and pag-\\ning techniques in operating systems. On top of it, we build\\nvLLM, an LLM serving system that achieves (1) near-zero\\nwaste in KV cache memory and (2) flexible sharing of KV',\n",
       " 'ing techniques in operating systems. On top of it, we build\\nvLLM, an LLM serving system that achieves (1) near-zero\\nwaste in KV cache memory and (2) flexible sharing of KV\\ncache within and across requests to further reduce mem-\\nory usage. Our evaluations show that vLLM improves the\\nthroughput of popular LLMs by 2-4 ×with the same level\\nof latency compared to the state-of-the-art systems, such\\nas FasterTransformer and Orca. The improvement is more\\npronounced with longer sequences, larger models, and more\\ncomplex decoding algorithms. vLLM’s source code is publicly\\navailable at https://github.com/vllm-project/vllm.\\n1 Introduction\\nThe emergence of large language models (LLMs) like GPT [5,\\n37] and PaLM [9] have enabled new applications such as pro-\\ngramming assistants [6, 18] and universal chatbots [19, 35]\\nthat are starting to profoundly impact our work and daily\\nroutines. Many cloud companies [34, 44] are racing to pro-\\nvide these applications as hosted services. However, running',\n",
       " 'that are starting to profoundly impact our work and daily\\nroutines. Many cloud companies [34, 44] are racing to pro-\\nvide these applications as hosted services. However, running\\nthese applications is very expensive, requiring a large num-\\nber of hardware accelerators such as GPUs. According to\\nrecent estimates, processing an LLM request can be 10×more\\nexpensive than a traditional keyword query [43]. Given these\\nhigh costs, increasing the throughput—and hence reducing\\nPermission to make digital or hard copies of part or all of this work for\\npersonal or classroom use is granted without fee provided that copies are\\nnot made or distributed for profit or commercial advantage and that copies\\nbear this notice and the full citation on the first page. Copyrights for third-\\nparty components of this work must be honored. For all other uses, contact\\nthe owner/author(s).\\nSOSP ’23, October 23–26, 2023, Koblenz, Germany\\n© 2023 Copyright held by the owner/author(s).\\nACM ISBN 979-8-4007-0229-7/23/10.',\n",
       " 'the owner/author(s).\\nSOSP ’23, October 23–26, 2023, Koblenz, Germany\\n© 2023 Copyright held by the owner/author(s).\\nACM ISBN 979-8-4007-0229-7/23/10.\\nhttps://doi.org/10.1145/3600006.3613165\\nNVIDIA A100 40GB\\nParameters \\n(26GB, 65%)\\nKV \\nCache\\n(>30%)\\nOthers\\n20\\n30\\n40Memory usage (GB)\\nParameter size\\nExisting systems vLLM\\n0 10 20 30 40\\nBatch size (# requests)\\n0\\n0.4k\\n0.8k\\n1.2kThroughput (token/s)\\nFigure 1. Left: Memory layout when serving an LLM with\\n13B parameters on NVIDIA A100. The parameters (gray)\\npersist in GPU memory throughout serving. The memory\\nfor the KV cache (red) is (de)allocated per serving request.\\nA small amount of memory (yellow) is used ephemerally\\nfor activation. Right: vLLM smooths out the rapid growth\\ncurve of KV cache memory seen in existing systems [31, 60],\\nleading to a notable boost in serving throughput.\\nthe cost per request—of LLM serving systems is becoming\\nmore important.\\nAt the core of LLMs lies an autoregressive Transformer',\n",
       " 'leading to a notable boost in serving throughput.\\nthe cost per request—of LLM serving systems is becoming\\nmore important.\\nAt the core of LLMs lies an autoregressive Transformer\\nmodel [53]. This model generates words (tokens), one at a\\ntime, based on the input (prompt) and the previous sequence\\nof the output’s tokens it has generated so far. For each re-\\nquest, this expensive process is repeated until the model out-\\nputs a termination token. This sequential generation process\\nmakes the workload memory-bound, underutilizing the com-\\nputation power of GPUs and limiting the serving throughput.\\nImproving the throughput is possible by batching multi-\\nple requests together. However, to process many requests\\nin a batch, the memory space for each request should be\\nefficiently managed. For example, Fig. 1 (left) illustrates the\\nmemory distribution for a 13B-parameter LLM on an NVIDIA\\nA100 GPU with 40GB RAM. Approximately 65% of the mem-',\n",
       " 'efficiently managed. For example, Fig. 1 (left) illustrates the\\nmemory distribution for a 13B-parameter LLM on an NVIDIA\\nA100 GPU with 40GB RAM. Approximately 65% of the mem-\\nory is allocated for the model weights, which remain static\\nduring serving. Close to 30% of the memory is used to store\\nthe dynamic states of the requests. For Transformers, these\\nstates consist of the key and value tensors associated with the\\nattention mechanism, commonly referred to asKV cache [41],\\nwhich represent the context from earlier tokens to gener-\\nate new output tokens in sequence. The remaining small\\n∗Equal contribution.\\n1\\narXiv:2309.06180v1  [cs.LG]  12 Sep 2023',\n",
       " 'Orca\\n(Max)\\nOrca\\n(Pow2)\\nOrca\\n(Oracle)\\nvLLM\\n0\\n20\\n40\\n60\\n80\\n100KV cache usage (%)\\n20.4\\n13.3\\n57.3\\n8.9\\n26.8\\n17.9\\n13.6\\n41.6\\n38.2\\n25.2\\n36.6\\n96.3\\nT oken states Reservation Internal frag. External frag.\\n& Others\\nFigure 2. Average percentage of memory wastes in different\\nLLM serving systems during the experiment in §6.2.\\npercentage of memory is used for other data, including ac-\\ntivations – the ephemeral tensors created when evaluating\\nthe LLM. Since the model weights are constant and the ac-\\ntivations only occupy a small fraction of the GPU memory,\\nthe way the KV cache is managed is critical in determining\\nthe maximum batch size. When managed inefficiently, the\\nKV cache memory can significantly limit the batch size and\\nconsequently the throughput of the LLM, as illustrated in\\nFig. 1 (right).\\nIn this paper, we observe that existing LLM serving sys-\\ntems [31, 60] fall short of managing the KV cache memory\\nefficiently. This is mainly because they store the KV cache of',\n",
       " 'Fig. 1 (right).\\nIn this paper, we observe that existing LLM serving sys-\\ntems [31, 60] fall short of managing the KV cache memory\\nefficiently. This is mainly because they store the KV cache of\\na request in contiguous memory space, as most deep learning\\nframeworks [33, 39] require tensors to be stored in contigu-\\nous memory. However, unlike the tensors in the traditional\\ndeep learning workloads, the KV cache has unique charac-\\nteristics: it dynamically grows and shrinks over time as the\\nmodel generates new tokens, and its lifetime and length are\\nnot known a priori. These characteristics make the existing\\nsystems’ approach significantly inefficient in two ways:\\nFirst, the existing systems [31, 60] suffer from internal and\\nexternal memory fragmentation. To store the KV cache of\\na request in contiguous space, they pre-allocate a contigu-\\nous chunk of memory with the request’s maximum length\\n(e.g., 2048 tokens). This can result in severe internal frag-',\n",
       " 'a request in contiguous space, they pre-allocate a contigu-\\nous chunk of memory with the request’s maximum length\\n(e.g., 2048 tokens). This can result in severe internal frag-\\nmentation, since the request’s actual length can be much\\nshorter than its maximum length (e.g., Fig. 11). Moreover,\\neven if the actual length is known a priori, the pre-allocation\\nis still inefficient: As the entire chunk is reserved during the\\nrequest’s lifetime, other shorter requests cannot utilize any\\npart of the chunk that is currently unused. Besides, external\\nmemory fragmentation can also be significant, since the pre-\\nallocated size can be different for each request. Indeed, our\\nprofiling results in Fig. 2 show that only 20.4% - 38.2% of the\\nKV cache memory is used to store the actual token states in\\nthe existing systems.\\nSecond, the existing systems cannot exploit the opportu-\\nnities for memory sharing. LLM services often use advanced\\ndecoding algorithms, such as parallel sampling and beam',\n",
       " 'the existing systems.\\nSecond, the existing systems cannot exploit the opportu-\\nnities for memory sharing. LLM services often use advanced\\ndecoding algorithms, such as parallel sampling and beam\\nsearch, that generate multiple outputs per request. In these\\nscenarios, the request consists of multiple sequences that can\\npartially share their KV cache. However, memory sharing is\\nnot possible in the existing systems because the KV cache of\\nthe sequences is stored in separate contiguous spaces.\\nTo address the above limitations, we propose PagedAt-\\ntention, an attention algorithm inspired by the operating\\nsystem’s (OS) solution to memory fragmentation and shar-\\ning: virtual memory with paging . PagedAttention divides the\\nrequest’s KV cache into blocks, each of which can contain\\nthe attention keys and values of a fixed number of tokens. In\\nPagedAttention, the blocks for the KV cache are not neces-\\nsarily stored in contiguous space. Therefore, we can manage']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert the text to embeddings\n",
    "texts = [doc.page_content for doc in docs_chunks]\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fc6c50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 5/5 [00:01<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 144 documents to the vector store...\n",
      "Documents added successfully. Total documents in collection: 288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## generate embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "embeddings.shape\n",
    "\n",
    "## strore in vectorstore\n",
    "vectorstore.add_documents(docs_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a0f608",
   "metadata": {},
   "source": [
    "### Rag Retrieval from Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4df964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "        \n",
    "        \n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str,Any]]:\n",
    "        # generate embedding for the query\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # perform similarity search in vector store \n",
    "        try : \n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "        \n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    similarity_score = 1 - distance  # convert distance to similarity score\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            \"id\": doc_id,\n",
    "                            \"document\": document,\n",
    "                            \"metadata\": metadata,\n",
    "                            \"similarity_score\": similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i+1\n",
    "                        })\n",
    "                        \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents for the query: '{query}'\")\n",
    "            else:\n",
    "                print(f\"No documents retrieved for the query: '{query}'\")\n",
    "                \n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "            \n",
    "rag_retriever = RAGRetriever(vectorstore, embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "408e3a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x31e9e8a00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80b97b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 5 documents for the query: 'What is memory management in Large Language Models?'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is memory management in Large Language Models?\"\n",
    "retrieved_li = rag_retriever.retrieve(query)\n",
    "len(retrieved_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ae485cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the decoding process advances.\\nScheduling for unknown input & output lengths. The\\nrequests to an LLM service exhibit variability in their input\\nand output lengths. This requires the memory management\\nsystem to accommodate a wide range of prompt lengths. In\\naddition, as the output length of a request grows at decoding,\\nthe memory required for its KV cache also expands and may\\nexhaust available memory for incoming requests or ongoing\\ngeneration for existing prompts. The system needs to make\\nscheduling decisions, such as deleting or swapping out the\\nKV cache of some requests from GPU memory.\\n3.1 Memory Management in Existing Systems\\nSince most operators in current deep learning frameworks\\n[33, 39] require tensors to be stored in contiguous memory,\\nprevious LLM serving systems [ 31, 60] also store the KV\\ncache of one request as a contiguous tensor across the differ-\\nent positions. Due to the unpredictable output lengths from\\nthe LLM, they statically allocate a chunk of memory for a'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_li[4]['document']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70630b4",
   "metadata": {},
   "source": [
    "### Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG Pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize Groq LLM\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(api_key=groq_api_key, model=\"gemma2-9b-it\",temperature=0.1, max_tokens=1024)\n",
    "\n",
    "### Simple RAG function : retrieve + generate\n",
    "def rag_simple(query, retriever, llm, top_k=3):\n",
    "    # retrieve relevant documents\n",
    "    results = retriever.retrieve(query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['document'] for doc in results]) if results else \"No relevant documents found.\"\n",
    "    if not context: \n",
    "        return \"No relevant documents found.\"\n",
    "    \n",
    "    prompt = f\"\"\"Use the following context to answer the question conscisely.\n",
    "    Context: \n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Answer:\"\"\".strip()\n",
    "    \n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0cda4754",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_simple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mrag_simple\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain memory management in LLMs\u001b[39m\u001b[38;5;124m\"\u001b[39m, rag_retriever, llm, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rag_simple' is not defined"
     ]
    }
   ],
   "source": [
    "answer = rag_simple(\"Explain memory management in LLMs\", rag_retriever, llm, top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c464ee47",
   "metadata": {},
   "source": [
    "### Enhanced RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d7dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_advanced(query, retriever, llm, top_k=5, min_score = 0.2, return_context = False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return \"No relevant documents found.\"   \n",
    "    \n",
    "    context = \"\\n\\n\".join([doc['document'] for doc in results])\n",
    "    \n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page' : doc['metadata'].get('page', doc['metadata'].get('pages', 'unknown')),\n",
    "        'score' : doc['similarity_score'],\n",
    "        'preview' : doc['document'][:300] + \"...\" if len(doc['document']) > 300 else doc['document']\n",
    "    } for doc in results]\n",
    "    \n",
    "    confidence = max(doc['similarity_score'] for doc in results)\n",
    "    \n",
    "    prompt = f\"\"\"Use the following context to answer the question conscisely.\n",
    "    Context: \n",
    "    {context}\n",
    "    Question: {query}\n",
    "    Answer:\"\"\".strip()\n",
    "    \n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        \"answer\": response.content,\n",
    "        \"confidence\": confidence,\n",
    "        \"sources\": sources\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "        \n",
    "    return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9536a3a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rag_advanced' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrag_advanced\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplain memory management in Large Language Models?\u001b[39m\u001b[38;5;124m\"\u001b[39m, rag_retriever, llm, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, return_context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfidence:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rag_advanced' is not defined"
     ]
    }
   ],
   "source": [
    "result = rag_advanced(\"Explain memory management in Large Language Models?\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Context Preview:\", result['context'][:300])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde45f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
